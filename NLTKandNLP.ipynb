{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Petite introduction au TAL avec nltk (et autres)\n",
    "\n",
    "## Stockage des XML\n",
    "#### On va commencer par stocker nos xml dans des String, hugoIntegral, stendhalIntegral, flaubertIntegral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "treeHugo = etree.parse(\"hugo_miserables.xml\")\n",
    "treeStendhal = etree.parse(\"stendhal_lerougeetlenoir.xml\")\n",
    "treeFlaubert=etree.parse(\"flaubert_bovary.xml\")\n",
    "listeHugo=list()\n",
    "listeStendhal=list()\n",
    "listeFlaubert=list()\n",
    "listeHugo.extend(user.text for user in treeHugo.xpath(\"/TEI/text/body/div/div/div/p\"))\n",
    "listeFlaubert.extend(user.text for user in treeFlaubert.xpath(\"/TEI/text/body/div/div/div/p\"))\n",
    "listeStendhal.extend(user.text for user in treeStendhal.xpath(\"/TEI/text/body/div/div/div/p\"))\n",
    "hugoIntegral=''\n",
    "stendhalIntegral=''\n",
    "flaubertIntegral=''\n",
    "for e in listeHugo:\n",
    "    if (isinstance(e,str)):\n",
    "        hugoIntegral+=e+' '\n",
    "for e in listeStendhal:\n",
    "    if (isinstance(e,str)):\n",
    "        stendhalIntegral+=e+' '\n",
    "for e in listeFlaubert:\n",
    "    if (isinstance(e,str)):\n",
    "        flaubertIntegral+=e+' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokénization et nettoyage\n",
    "#### Ici on utilise un word-tokenizer basique, basé sur le Penn Bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Je', 'suis', 'Marianne', 'et', 'je', 'fais', 'un', 'simple', 'test']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "#nltk.download()\n",
    "# chargement du tokenizer\n",
    "#tokenizer = nltk.data.load('tokenizers/punkt/french.pickle')\n",
    "strTest='Je suis Marianne et je fais un simple test'\n",
    "tokenizer=TreebankWordTokenizer()\n",
    "#tokenizer = nltk.data.load('./nltk_data/tokenizers/punkt/french.pickle')\n",
    "tokens = tokenizer.tokenize(strTest)\n",
    "print (tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ici, on va supprimer les stopwords qui faussent souvent les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Je', 'Marianne', 'simple', 'test']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#stop_words=set(stopwords.words(\"french\"))\n",
    "with open('stopwords.txt', encoding='utf-8') as f:\n",
    "    stop_words = f.read().split('\\n')\n",
    "#texte_sans_SW=[]\n",
    "#for t in tokens :\n",
    "#    if t not in stop_words :\n",
    "#        texte_sans_SW.append(t)\n",
    "texte_sans_SW=[t for t in tokens if not t in stop_words]\n",
    "print (texte_sans_SW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Racinisation, lemmatisation et étiquetage\n",
    "#### Souvent, pour l'analyse, on utilise un outil léger, un stemmer qui réduit chaque mot à sa racine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'suis', 'mariann', 'et', 'je', 'fais', 'un', 'simpl', 'test']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import FrenchStemmer\n",
    "st=FrenchStemmer()\n",
    "stemmed_text=[st.stem(mot) for mot in texte_sans_SW]\n",
    "print (stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stringATagger=''\n",
    "for tok in texte_sans_SW:\n",
    "    stringATagger+=tok+' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mais souvent, il est plus pertinent de lemmatiser et de postagger un texte. La lemmatisation ici est faite avec un wrapper (un adaptateur) du lemmatiseur TreeTagger, et l'étiquetage syntaxique léger avec le Stanford POStagger.\n",
    "\n",
    "Attention, vous aurez besoin ici d'un wrapper, que vous obtiendrez en exécutant cette commande :\n",
    "    pip install treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['je', 'suivre|être', 'Marianne', 'et', 'je', 'faire', 'un', 'simple', 'test']\n"
     ]
    }
   ],
   "source": [
    "import treetaggerwrapper\n",
    "import re\n",
    "#1) build a TreeTagger wrapper:\n",
    "tagger = treetaggerwrapper.TreeTagger(TAGDIR='/home/odysseus/Bureau/PythonCourse/treetagger/',TAGPARFILE='/home/odysseus/Bureau/PythonCourse/treetagger/models/french.par')\n",
    "#2) tag your text.\n",
    "tags = tagger.tag_text(stringATagger)\n",
    "lems=list()\n",
    "for wordTagged in tags:\n",
    "    lems.append(re.split(r'\\t+', wordTagged)[2])\n",
    "print(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Je', 'CLS'), ('suis', 'V'), ('Marianne', 'NPP'), ('et', 'CC'), ('je', 'CLS'), ('fais', 'V'), ('un', 'DET'), ('simple', 'ADJ'), ('test', 'NC')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordPOSTagger\n",
    "jar = 'stanford/stanford-postagger-3.7.0.jar'\n",
    "model = 'models/french.tagger'\n",
    "pos_tagger = StanfordPOSTagger(model, jar, encoding='utf8')\n",
    "tags=pos_tagger.tag(stringATagger.split())\n",
    "print (tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarité de documents\n",
    "#### Ici ça va vous sembler répétitif, mais on va refaire le même procédé (sans les prints) sur nos trois xml, pour voir, entre autre, la proximité des documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synopses=list()\n",
    "synopses.append(hugoIntegral)\n",
    "synopses.append(flaubertIntegral)\n",
    "synopses.append(stendhalIntegral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Une des principales mesures de proximité entre documents, et qui exige une bonne tokenization, voire lemmatisation, est la TF-IDF (Term Frequency-Inverse Document Frequency). Le principe de la TF-IDF est de transformer les documents en des vecteurs dont le poids des mots est déterminé en fonction de leur fréquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_my_way(text):\n",
    "    filtered_tokens = []\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-ZéèêàâîôûçïöüäëÉ]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stops=list()\n",
    "for sw in stop_words:\n",
    "    stops.append(sw)\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=1, max_features=200000,\n",
    "                                 #min_df=0.0, stop_words=stops,\n",
    "                                 #use_idf=True, tokenizer=tokenize_my_way, ngram_range=(1,3))\n",
    "\n",
    "tfidf_matrix = TfidfVectorizer().fit_transform(synopses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
